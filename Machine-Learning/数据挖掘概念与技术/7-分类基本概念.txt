1、分类的一般方法
数据分类是两阶段过程，包括学习阶段(构建分类模型)和分类阶段(使用模型预测给定数据的类标号)
学习：用分类算法分析训练数据，学习的模型或分类器以分类规则形式提供
分类：检验数据用于评估分类规则的准确率，如果准确率可以被接受，则规则用于新的数据元组分类


监督学习：分类器的学习在被告知每个训练元组属于哪个类的监督下进行的


2、决策树归纳：是从有类标号的训练元组中学习决策树
决策树是一种类似于流程图的树结构，其中每个内部节点(非树叶节点)表示在一个属性上的测试，每个分支代表该测试的一个输出；每个树叶节点存放一个类标号；输的最顶层节点是根节点。
2.1、决策树归纳：A是分裂属性
如果A是离散值，如low,medium,high，则对A的每个已知值产生一个分支
如果A是连续值，如《10000，>10000，则对应的产生两个分支
如果A是离散值，但必须产生二叉树，则测试形如A属于SA，其中SA是A的分裂子集


2.1、属性选择度量：是一种分裂准则，把给定类标记的训练元组的数据分区D最好地划分成单独类的启发式方法
三种常用的属性选择度量(分裂规则)：信息增益、增益率、基尼指数(Gini指数)

例:数据分区D为标记类元素的训练集，假定类标号属性具有m个不同值，定义了m个不同的类ci(i=1,2,...m)，设ci,D是D中ci类元组的集合，|D|和|ci,D|分别是D和ci,D中元组的个数

2.1.1、信息增益：选择具有最高信息增益的属性作为节点N的分裂属性，该属性使结果分区中对元组分类所需要的信息量最小，并反应这些分区中的最小随机性或不纯性，该方法使得对一个对象分类所需要的期望测试数目最小，并确保找到一颗简单的树。

2.1.1.1对于离散型属性时，info(D)=-∑pilog2下标(pi)，pi是D中任意元组属于类ci的概率(非0)，并用|ci,D|/|D|表示，使用以2为底的对数是因为信息用二进制编码，info(D)是识别D中元组的类标号所需要的平均信息量，称为D的熵。

假设按某属性A划分D中的元组，A一个有v个不同值，如果A是离散值，则可以使用属性A将D划分为v个分区。正常情况下，我们希望该划分产生的元组的准确分类，即每个分区都是纯的，但实际上，分区可能包含来自不同类而不是来自单个类的元组。在此划分之后，为了得到准确的分类，我们还需要多少信息，这个量可以由下式度量：infoA(D)=∑(|Dj|/|D|)*info(Dj),其中|Dj|/|D|充当第j个分区的权重，infoA(D)是基于按A划分对D的元组分类所需要的期望信息，需要的期望信息越小，分区纯度越高。

综上所述，信息增益定义为原来的信息需求(仅基于类比例)与新的信息需求(对A划分后)之间的差，即:
Gain(A)=info(D)-infoA(D),换言之，Gain(A)告诉我们通过A上的划分我们得到了多少，它是知道A的值而导致的信息需求的期望减少，选择具有最高信息增益Gain的属性A作为节点N的分裂属性，这等价于在能做最佳分类的属性A上划分，使得完成元组分类还需要的信息最小。

2.1.1.2、对于连续性属性时，首先将A的值按递增排序，每对相邻值的钟点被看作可能的分裂点，对于给定A的v个值，需要v-1个可能的划分。
如果A的值已经预先排序，则确定A的最佳划分只需要扫描一遍这些值，对于A的每个可能分裂点，计算infoA(D)


2.2、增益率
SplitInfoA(D)=-∑(|Dj|/|D|)*log2(|Dj|/|D|)
增益率GrianRate(A)=Grain(A)/SplitInfoA(D)


2.3、基尼指数
不纯度Gini(D)=1-∑pi²，pi是D中元组属于Ci的概率


2.4、树剪枝
在决策树创建时，由于噪声和离群点，许多分支反映的是训练数据中的异常，剪枝方法可以处理这种过分拟合数据问题，通常，这种方法使用统计度量剪掉最不可靠的分支，剪枝后，决策树更为简单。

树剪枝分为先剪枝和后剪枝
后剪枝更常见，剪去后使用子树种最频繁的类替换


3、贝叶斯方法：统计学分类方法，可以预测类隶属关系的概率，如一个给定的元组属于一个特定类的概率
P(H|X)是后验概率，如H是购买计算机的假设，X是顾客收入和年龄，那么P(H|X)反映当知道顾客收入和年龄时，购买计算机的概率
P(H)是先验概率，任意给定顾客将购买计算机的概率，而不管他们年龄，收入等其他信息，P(H)独立于X
贝叶斯定理P(H|X)=(P(X|H)P(H))/P(X)


3.1、朴素贝叶斯(简单贝叶斯)分类法：
3.11设D是训练元组和相关联的类标号的集合，X是n维属性，一共有n个属性
3.12假定有m个类c1,c2..cm，分类法将预测X属于具有最高后验概率的类(在条件X下)，最大化P(Ci|X)>P(Cj|X),
3.13P(Ci|X)最大的类ci为最大后验假设，根据贝叶斯定理P(Ci|X)=(P(X|Ci)P(Ci))/P(X),因为P(x)对所有类为常数，因此只用(P(X|Ci)P(Ci))最大即可
3.14计算P(X|Ci)可能较复杂，因此可以做类条件独立的朴素假定，假定属性值有条件地相互独立，因此P(X|Ci)=P(X1|Ci)P(X2|Ci)..P(Xn|Ci)
其中，如果Xn是分类属性，则P(Xn|Ci)是D中属性的值=ci类的元组数除以D中Ci类的元组数|ci，D|
      如果Xn是连续值属性免责可能较为复杂
3.15为了预测X的类标号，对每个类ci，计算P(X|Ci)P(Ci),也就是最大化的该值



4、基于规则的分类
4.1、使用if-then规则分类
if条件 then结论：R1：IF age = youth AND student=yes THEN boys_computer=yes:(age=youth)^(student=yes)=>(buys_computer=yes)
该规则R1可以使用覆盖率和准确率来评估：ncovers为规则覆盖的元组数，ncorrect为正确分类的元组数，|D|是D中的元组数
coverage(R)=ncovers/|D|,acuracy(R)=ncorrect/ncovers

4.11、如果规则被X满足，则称该规则被触发
假设X=(age=youth,income=medium,student=yes,credit_rating=fair)，想根据buys_computer对分类，X满足R1,触发该规则。
如果R1是唯一满足的规则，则该规则激活，返回X的类预测，但是可能存在多个规则被满足，或者没有一个规则被X满足。。

4.12，针对多个规则被满足的问题，可以使用两种方法来就解决冲突：规模序和规则序
规模序将最高优先权赋予具有最苛刻要求的被触发的规则，即激活具有最多属性测试的被触发规则

规则序方案预先确定规则的优先排序，这种序可以是基于类或基于规则的，基于类的序，类按照重要性递减排序；基于规则的序，根据规则的度量，如准确率、覆盖率或规模，或根据领域专家的建议，把规则组织成一个优先权列表。在使用规则序时，规则集称为决策表，使用规则序，最先出现在决策表中的被触发的规则具有最高优先权，因此激活它的类预测。

4.13，针对没有一个规则被X满足的情况，可以建立一个省缺或默认规则，根据训练集指定一个默认类，这个类可以是多数类，或者不被任何规则覆盖的元组的多数类，当且仅当没有其他规则覆盖X时，最后才使用默认规则。默认规则的条件为空，这样，当没有其他规则满足时该规则被激活。

4.2、由决策树提取规则
就是对决策树使用if-then


4.3、使用顺序覆盖算法的规则归纳
使用顺序覆盖算法可以直接从训练数据提取if-then规则，不必产生决策树。
一次学习一个规则，然后删除该规则覆盖的元组，并在剩余的元组中重复该过程。
















